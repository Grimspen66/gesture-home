{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T01:32:07.043144Z",
     "start_time": "2025-04-07T01:32:05.172985Z"
    }
   },
   "source": [
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "RANDOM_SEED = 42"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 各パス指定"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T01:32:07.121511Z",
     "start_time": "2025-04-07T01:32:07.047147Z"
    }
   },
   "source": [
    "dataset = 'model/keypoint_classifier/keypoint.csv'\n",
    "\n",
    "# Define the path where the model checkpoint will be saved (with proper path format)\n",
    "model_save_path = r'grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras'\n",
    "\n",
    "# モデルチェックポイントのコールバック\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    model_save_path, verbose=1, save_weights_only=False)\n",
    "\n",
    "# 早期打ち切り用コールバック\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(patience=20, verbose=1)\n",
    "\n",
    "# You can now use these callbacks during model training:\n",
    "# model.fit(X_train, y_train, epochs=100, callbacks=[cp_callback, es_callback])\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分類数設定"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T01:32:07.436206Z",
     "start_time": "2025-04-07T01:32:07.421206Z"
    }
   },
   "source": "NUM_CLASSES = 6",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習データ読み込み"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T01:32:07.498522Z",
     "start_time": "2025-04-07T01:32:07.454210Z"
    }
   },
   "source": [
    "X_dataset = np.loadtxt(dataset, delimiter=',', dtype='float32', usecols=list(range(1, (21 * 2) + 1)))"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T01:32:07.530529Z",
     "start_time": "2025-04-07T01:32:07.515530Z"
    }
   },
   "source": "y_dataset = np.loadtxt(dataset, delimiter=',', dtype='int32', usecols=(0))",
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T01:32:07.562532Z",
     "start_time": "2025-04-07T01:32:07.547533Z"
    }
   },
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_dataset, y_dataset, train_size=0.75, random_state=RANDOM_SEED)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデル構築"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T01:32:07.625657Z",
     "start_time": "2025-04-07T01:32:07.579540Z"
    }
   },
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input((21 * 2, )),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(20, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(10, activation='relu'),\n",
    "    tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "])"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T01:32:07.735177Z",
     "start_time": "2025-04-07T01:32:07.642660Z"
    }
   },
   "source": [
    "model.summary()  # tf.keras.utils.plot_model(model, show_shapes=True)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"sequential\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dropout (\u001B[38;5;33mDropout\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m42\u001B[0m)             │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001B[38;5;33mDense\u001B[0m)                   │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m20\u001B[0m)             │           \u001B[38;5;34m860\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001B[38;5;33mDropout\u001B[0m)             │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m20\u001B[0m)             │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m10\u001B[0m)             │           \u001B[38;5;34m210\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m6\u001B[0m)              │            \u001B[38;5;34m66\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">860</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">210</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m1,136\u001B[0m (4.44 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,136</span> (4.44 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m1,136\u001B[0m (4.44 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,136</span> (4.44 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T01:32:07.766179Z",
     "start_time": "2025-04-07T01:32:07.752178Z"
    }
   },
   "source": [
    "# モデルチェックポイントのコールバック\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    model_save_path, verbose=1, save_weights_only=False)\n",
    "# 早期打ち切り用コールバック\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(patience=20, verbose=1)"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T01:32:07.797220Z",
     "start_time": "2025-04-07T01:32:07.782184Z"
    }
   },
   "source": [
    "# モデルコンパイル\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデル訓練"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-04-07T01:32:22.951110Z",
     "start_time": "2025-04-07T01:32:07.813608Z"
    }
   },
   "source": [
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=1000,\n",
    "    batch_size=128,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[cp_callback, es_callback]\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m10s\u001B[0m 391ms/step - accuracy: 0.2031 - loss: 1.8582\n",
      "Epoch 1: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.2346 - loss: 1.7867 - val_accuracy: 0.3752 - val_loss: 1.6365\n",
      "Epoch 2/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.3281 - loss: 1.6746\n",
      "Epoch 2: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.3264 - loss: 1.6345 - val_accuracy: 0.4312 - val_loss: 1.5136\n",
      "Epoch 3/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.3047 - loss: 1.5937\n",
      "Epoch 3: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.3706 - loss: 1.5399 - val_accuracy: 0.5129 - val_loss: 1.3903\n",
      "Epoch 4/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.4297 - loss: 1.4812\n",
      "Epoch 4: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.4189 - loss: 1.4393 - val_accuracy: 0.5680 - val_loss: 1.2911\n",
      "Epoch 5/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.4609 - loss: 1.3419\n",
      "Epoch 5: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.4306 - loss: 1.3669 - val_accuracy: 0.5783 - val_loss: 1.2160\n",
      "Epoch 6/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.5391 - loss: 1.2843\n",
      "Epoch 6: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.4702 - loss: 1.3001 - val_accuracy: 0.5878 - val_loss: 1.1506\n",
      "Epoch 7/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.5000 - loss: 1.2149\n",
      "Epoch 7: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.4694 - loss: 1.2643 - val_accuracy: 0.6179 - val_loss: 1.0986\n",
      "Epoch 8/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.5156 - loss: 1.1944\n",
      "Epoch 8: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.5048 - loss: 1.2167 - val_accuracy: 0.6489 - val_loss: 1.0489\n",
      "Epoch 9/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.4922 - loss: 1.3093\n",
      "Epoch 9: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.5100 - loss: 1.1991 - val_accuracy: 0.6523 - val_loss: 1.0065\n",
      "Epoch 10/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.5234 - loss: 1.1329\n",
      "Epoch 10: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.5074 - loss: 1.1542 - val_accuracy: 0.6532 - val_loss: 0.9697\n",
      "Epoch 11/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.5625 - loss: 1.0692\n",
      "Epoch 11: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.5286 - loss: 1.1447 - val_accuracy: 0.6566 - val_loss: 0.9418\n",
      "Epoch 12/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.4766 - loss: 1.1607\n",
      "Epoch 12: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.5491 - loss: 1.1235 - val_accuracy: 0.7212 - val_loss: 0.9057\n",
      "Epoch 13/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.5469 - loss: 1.0851\n",
      "Epoch 13: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.5515 - loss: 1.0980 - val_accuracy: 0.7315 - val_loss: 0.8763\n",
      "Epoch 14/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.5703 - loss: 1.0917\n",
      "Epoch 14: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.5622 - loss: 1.0640 - val_accuracy: 0.7349 - val_loss: 0.8491\n",
      "Epoch 15/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.6016 - loss: 0.9433\n",
      "Epoch 15: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.5726 - loss: 1.0341 - val_accuracy: 0.7410 - val_loss: 0.8234\n",
      "Epoch 16/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.5781 - loss: 1.0564\n",
      "Epoch 16: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.5815 - loss: 1.0361 - val_accuracy: 0.7410 - val_loss: 0.7998\n",
      "Epoch 17/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.5625 - loss: 1.1101\n",
      "Epoch 17: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.5799 - loss: 1.0349 - val_accuracy: 0.7410 - val_loss: 0.7795\n",
      "Epoch 18/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.5938 - loss: 0.9689\n",
      "Epoch 18: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6052 - loss: 0.9979 - val_accuracy: 0.7556 - val_loss: 0.7609\n",
      "Epoch 19/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.6797 - loss: 0.9491\n",
      "Epoch 19: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6192 - loss: 0.9721 - val_accuracy: 0.7556 - val_loss: 0.7409\n",
      "Epoch 20/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.5781 - loss: 0.9907\n",
      "Epoch 20: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6130 - loss: 0.9839 - val_accuracy: 0.7539 - val_loss: 0.7239\n",
      "Epoch 21/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.6016 - loss: 1.0174\n",
      "Epoch 21: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6171 - loss: 0.9808 - val_accuracy: 0.7754 - val_loss: 0.7056\n",
      "Epoch 22/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.5781 - loss: 1.0118\n",
      "Epoch 22: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6163 - loss: 0.9518 - val_accuracy: 0.7806 - val_loss: 0.6868\n",
      "Epoch 23/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.6094 - loss: 0.9426\n",
      "Epoch 23: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6464 - loss: 0.9330 - val_accuracy: 0.7978 - val_loss: 0.6692\n",
      "Epoch 24/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.5625 - loss: 1.0196\n",
      "Epoch 24: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6322 - loss: 0.9297 - val_accuracy: 0.7995 - val_loss: 0.6616\n",
      "Epoch 25/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.6641 - loss: 0.8983\n",
      "Epoch 25: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6306 - loss: 0.9202 - val_accuracy: 0.7935 - val_loss: 0.6470\n",
      "Epoch 26/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.6484 - loss: 0.8380\n",
      "Epoch 26: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6354 - loss: 0.9170 - val_accuracy: 0.7995 - val_loss: 0.6350\n",
      "Epoch 27/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7422 - loss: 0.7610\n",
      "Epoch 27: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6436 - loss: 0.8954 - val_accuracy: 0.8150 - val_loss: 0.6187\n",
      "Epoch 28/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.6016 - loss: 0.9117\n",
      "Epoch 28: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6634 - loss: 0.8719 - val_accuracy: 0.8158 - val_loss: 0.6031\n",
      "Epoch 29/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.6094 - loss: 0.9059\n",
      "Epoch 29: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6440 - loss: 0.8866 - val_accuracy: 0.8244 - val_loss: 0.5950\n",
      "Epoch 30/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.5938 - loss: 0.9291\n",
      "Epoch 30: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6517 - loss: 0.8803 - val_accuracy: 0.8305 - val_loss: 0.5826\n",
      "Epoch 31/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.6016 - loss: 0.8839\n",
      "Epoch 31: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6588 - loss: 0.8540 - val_accuracy: 0.8348 - val_loss: 0.5723\n",
      "Epoch 32/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.6641 - loss: 0.8743\n",
      "Epoch 32: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6526 - loss: 0.8670 - val_accuracy: 0.8305 - val_loss: 0.5630\n",
      "Epoch 33/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.6094 - loss: 0.9990\n",
      "Epoch 33: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6605 - loss: 0.8517 - val_accuracy: 0.8408 - val_loss: 0.5448\n",
      "Epoch 34/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7109 - loss: 0.8068\n",
      "Epoch 34: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6727 - loss: 0.8343 - val_accuracy: 0.8434 - val_loss: 0.5375\n",
      "Epoch 35/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.6953 - loss: 0.8557\n",
      "Epoch 35: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6901 - loss: 0.8066 - val_accuracy: 0.8494 - val_loss: 0.5259\n",
      "Epoch 36/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.6719 - loss: 0.8049\n",
      "Epoch 36: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6569 - loss: 0.8250 - val_accuracy: 0.8468 - val_loss: 0.5155\n",
      "Epoch 37/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.6953 - loss: 0.8077\n",
      "Epoch 37: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6761 - loss: 0.8154 - val_accuracy: 0.8537 - val_loss: 0.5109\n",
      "Epoch 38/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7188 - loss: 0.7073\n",
      "Epoch 38: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6825 - loss: 0.7886 - val_accuracy: 0.8589 - val_loss: 0.5002\n",
      "Epoch 39/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7422 - loss: 0.7702\n",
      "Epoch 39: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6793 - loss: 0.8211 - val_accuracy: 0.8503 - val_loss: 0.4984\n",
      "Epoch 40/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.7109 - loss: 0.8079\n",
      "Epoch 40: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6851 - loss: 0.7898 - val_accuracy: 0.8503 - val_loss: 0.4901\n",
      "Epoch 41/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.6953 - loss: 0.7663\n",
      "Epoch 41: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6789 - loss: 0.8184 - val_accuracy: 0.8589 - val_loss: 0.4887\n",
      "Epoch 42/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7031 - loss: 0.7185\n",
      "Epoch 42: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7046 - loss: 0.7474 - val_accuracy: 0.8632 - val_loss: 0.4769\n",
      "Epoch 43/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7109 - loss: 0.7371\n",
      "Epoch 43: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6986 - loss: 0.7808 - val_accuracy: 0.8735 - val_loss: 0.4713\n",
      "Epoch 44/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.6953 - loss: 0.8347\n",
      "Epoch 44: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6944 - loss: 0.7750 - val_accuracy: 0.8657 - val_loss: 0.4664\n",
      "Epoch 45/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7109 - loss: 0.7636\n",
      "Epoch 45: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7002 - loss: 0.7671 - val_accuracy: 0.8709 - val_loss: 0.4525\n",
      "Epoch 46/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.6953 - loss: 0.8086\n",
      "Epoch 46: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7115 - loss: 0.7537 - val_accuracy: 0.8735 - val_loss: 0.4425\n",
      "Epoch 47/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7188 - loss: 0.7727\n",
      "Epoch 47: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7123 - loss: 0.7438 - val_accuracy: 0.8709 - val_loss: 0.4342\n",
      "Epoch 48/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7656 - loss: 0.5850\n",
      "Epoch 48: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7007 - loss: 0.7578 - val_accuracy: 0.8890 - val_loss: 0.4388\n",
      "Epoch 49/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7188 - loss: 0.8205\n",
      "Epoch 49: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7089 - loss: 0.7593 - val_accuracy: 0.8821 - val_loss: 0.4268\n",
      "Epoch 50/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.6484 - loss: 0.8416\n",
      "Epoch 50: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6965 - loss: 0.7558 - val_accuracy: 0.8787 - val_loss: 0.4253\n",
      "Epoch 51/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7578 - loss: 0.6631\n",
      "Epoch 51: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7139 - loss: 0.7293 - val_accuracy: 0.8881 - val_loss: 0.4171\n",
      "Epoch 52/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7422 - loss: 0.7197\n",
      "Epoch 52: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7032 - loss: 0.7491 - val_accuracy: 0.8976 - val_loss: 0.4164\n",
      "Epoch 53/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.6719 - loss: 0.7503\n",
      "Epoch 53: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7227 - loss: 0.7238 - val_accuracy: 0.8950 - val_loss: 0.4040\n",
      "Epoch 54/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 34ms/step - accuracy: 0.7734 - loss: 0.6965\n",
      "Epoch 54: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7255 - loss: 0.7256 - val_accuracy: 0.9002 - val_loss: 0.4023\n",
      "Epoch 55/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 19ms/step - accuracy: 0.6328 - loss: 0.8631\n",
      "Epoch 55: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7152 - loss: 0.7355 - val_accuracy: 0.8976 - val_loss: 0.3992\n",
      "Epoch 56/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7422 - loss: 0.6186\n",
      "Epoch 56: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7322 - loss: 0.6887 - val_accuracy: 0.9114 - val_loss: 0.3900\n",
      "Epoch 57/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7109 - loss: 0.8086\n",
      "Epoch 57: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7209 - loss: 0.7228 - val_accuracy: 0.9122 - val_loss: 0.3883\n",
      "Epoch 58/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7578 - loss: 0.5965\n",
      "Epoch 58: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7405 - loss: 0.6989 - val_accuracy: 0.9053 - val_loss: 0.3824\n",
      "Epoch 59/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7266 - loss: 0.7023\n",
      "Epoch 59: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7173 - loss: 0.7465 - val_accuracy: 0.9148 - val_loss: 0.3846\n",
      "Epoch 60/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7344 - loss: 0.6516\n",
      "Epoch 60: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7283 - loss: 0.7035 - val_accuracy: 0.9071 - val_loss: 0.3754\n",
      "Epoch 61/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.6875 - loss: 0.6921\n",
      "Epoch 61: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7357 - loss: 0.6865 - val_accuracy: 0.9174 - val_loss: 0.3667\n",
      "Epoch 62/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7812 - loss: 0.6512\n",
      "Epoch 62: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7288 - loss: 0.7069 - val_accuracy: 0.9260 - val_loss: 0.3686\n",
      "Epoch 63/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7500 - loss: 0.6649\n",
      "Epoch 63: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7419 - loss: 0.6831 - val_accuracy: 0.9165 - val_loss: 0.3661\n",
      "Epoch 64/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7188 - loss: 0.7692\n",
      "Epoch 64: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7325 - loss: 0.7048 - val_accuracy: 0.9122 - val_loss: 0.3651\n",
      "Epoch 65/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7266 - loss: 0.7385\n",
      "Epoch 65: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7279 - loss: 0.7036 - val_accuracy: 0.9200 - val_loss: 0.3632\n",
      "Epoch 66/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7656 - loss: 0.5607\n",
      "Epoch 66: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7346 - loss: 0.6827 - val_accuracy: 0.9139 - val_loss: 0.3599\n",
      "Epoch 67/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7891 - loss: 0.6296\n",
      "Epoch 67: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7379 - loss: 0.6967 - val_accuracy: 0.9200 - val_loss: 0.3534\n",
      "Epoch 68/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.7500 - loss: 0.7527\n",
      "Epoch 68: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7229 - loss: 0.7117 - val_accuracy: 0.9200 - val_loss: 0.3477\n",
      "Epoch 69/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7891 - loss: 0.7847\n",
      "Epoch 69: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7465 - loss: 0.6864 - val_accuracy: 0.9243 - val_loss: 0.3472\n",
      "Epoch 70/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7656 - loss: 0.7225\n",
      "Epoch 70: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7452 - loss: 0.6903 - val_accuracy: 0.9251 - val_loss: 0.3453\n",
      "Epoch 71/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7500 - loss: 0.6907\n",
      "Epoch 71: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7332 - loss: 0.6949 - val_accuracy: 0.9303 - val_loss: 0.3446\n",
      "Epoch 72/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7031 - loss: 0.7499\n",
      "Epoch 72: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7478 - loss: 0.6780 - val_accuracy: 0.9286 - val_loss: 0.3437\n",
      "Epoch 73/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7656 - loss: 0.6485\n",
      "Epoch 73: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7472 - loss: 0.6539 - val_accuracy: 0.9329 - val_loss: 0.3288\n",
      "Epoch 74/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7500 - loss: 0.6294\n",
      "Epoch 74: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7493 - loss: 0.6633 - val_accuracy: 0.9449 - val_loss: 0.3229\n",
      "Epoch 75/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7344 - loss: 0.6618\n",
      "Epoch 75: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7498 - loss: 0.6739 - val_accuracy: 0.9441 - val_loss: 0.3313\n",
      "Epoch 76/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7812 - loss: 0.5738\n",
      "Epoch 76: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7566 - loss: 0.6539 - val_accuracy: 0.9389 - val_loss: 0.3255\n",
      "Epoch 77/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7422 - loss: 0.7673\n",
      "Epoch 77: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7556 - loss: 0.6532 - val_accuracy: 0.9475 - val_loss: 0.3217\n",
      "Epoch 78/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7344 - loss: 0.6526\n",
      "Epoch 78: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7499 - loss: 0.6605 - val_accuracy: 0.9432 - val_loss: 0.3215\n",
      "Epoch 79/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7578 - loss: 0.6546\n",
      "Epoch 79: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7505 - loss: 0.6706 - val_accuracy: 0.9398 - val_loss: 0.3178\n",
      "Epoch 80/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7656 - loss: 0.5725\n",
      "Epoch 80: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7589 - loss: 0.6338 - val_accuracy: 0.9406 - val_loss: 0.3098\n",
      "Epoch 81/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7969 - loss: 0.5650\n",
      "Epoch 81: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7575 - loss: 0.6452 - val_accuracy: 0.9441 - val_loss: 0.3083\n",
      "Epoch 82/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7578 - loss: 0.6175\n",
      "Epoch 82: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7636 - loss: 0.6282 - val_accuracy: 0.9501 - val_loss: 0.3053\n",
      "Epoch 83/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7344 - loss: 0.7500\n",
      "Epoch 83: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7653 - loss: 0.6613 - val_accuracy: 0.9492 - val_loss: 0.2988\n",
      "Epoch 84/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.7891 - loss: 0.5794\n",
      "Epoch 84: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7537 - loss: 0.6509 - val_accuracy: 0.9596 - val_loss: 0.2937\n",
      "Epoch 85/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7188 - loss: 0.6468\n",
      "Epoch 85: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7535 - loss: 0.6399 - val_accuracy: 0.9561 - val_loss: 0.3011\n",
      "Epoch 86/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7656 - loss: 0.5994\n",
      "Epoch 86: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7553 - loss: 0.6433 - val_accuracy: 0.9570 - val_loss: 0.2936\n",
      "Epoch 87/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7109 - loss: 0.7525\n",
      "Epoch 87: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7589 - loss: 0.6469 - val_accuracy: 0.9570 - val_loss: 0.2972\n",
      "Epoch 88/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7578 - loss: 0.6422\n",
      "Epoch 88: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7740 - loss: 0.6129 - val_accuracy: 0.9527 - val_loss: 0.2888\n",
      "Epoch 89/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7891 - loss: 0.5555\n",
      "Epoch 89: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7594 - loss: 0.6345 - val_accuracy: 0.9544 - val_loss: 0.2935\n",
      "Epoch 90/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7344 - loss: 0.6284\n",
      "Epoch 90: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7506 - loss: 0.6513 - val_accuracy: 0.9501 - val_loss: 0.2933\n",
      "Epoch 91/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.6719 - loss: 0.7132\n",
      "Epoch 91: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7531 - loss: 0.6450 - val_accuracy: 0.9484 - val_loss: 0.3018\n",
      "Epoch 92/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7734 - loss: 0.6180\n",
      "Epoch 92: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7723 - loss: 0.6284 - val_accuracy: 0.9501 - val_loss: 0.2897\n",
      "Epoch 93/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.7344 - loss: 0.6934\n",
      "Epoch 93: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7581 - loss: 0.6604 - val_accuracy: 0.9518 - val_loss: 0.2963\n",
      "Epoch 94/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7188 - loss: 0.7213\n",
      "Epoch 94: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7507 - loss: 0.6594 - val_accuracy: 0.9596 - val_loss: 0.2916\n",
      "Epoch 95/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.8047 - loss: 0.5452\n",
      "Epoch 95: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7578 - loss: 0.6310 - val_accuracy: 0.9535 - val_loss: 0.2898\n",
      "Epoch 96/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7578 - loss: 0.6491\n",
      "Epoch 96: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7680 - loss: 0.6237 - val_accuracy: 0.9587 - val_loss: 0.2816\n",
      "Epoch 97/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.6797 - loss: 0.7056\n",
      "Epoch 97: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7578 - loss: 0.6371 - val_accuracy: 0.9587 - val_loss: 0.2815\n",
      "Epoch 98/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7344 - loss: 0.6382\n",
      "Epoch 98: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7555 - loss: 0.6450 - val_accuracy: 0.9604 - val_loss: 0.2881\n",
      "Epoch 99/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.8359 - loss: 0.5643\n",
      "Epoch 99: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7698 - loss: 0.6209 - val_accuracy: 0.9613 - val_loss: 0.2829\n",
      "Epoch 100/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7734 - loss: 0.5383\n",
      "Epoch 100: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7617 - loss: 0.6301 - val_accuracy: 0.9630 - val_loss: 0.2784\n",
      "Epoch 101/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7656 - loss: 0.6154\n",
      "Epoch 101: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7700 - loss: 0.6402 - val_accuracy: 0.9561 - val_loss: 0.2816\n",
      "Epoch 102/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7969 - loss: 0.7120\n",
      "Epoch 102: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7725 - loss: 0.6373 - val_accuracy: 0.9621 - val_loss: 0.2783\n",
      "Epoch 103/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7812 - loss: 0.6977\n",
      "Epoch 103: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7693 - loss: 0.6250 - val_accuracy: 0.9647 - val_loss: 0.2794\n",
      "Epoch 104/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.8047 - loss: 0.5512\n",
      "Epoch 104: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7755 - loss: 0.5980 - val_accuracy: 0.9621 - val_loss: 0.2752\n",
      "Epoch 105/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7344 - loss: 0.6955\n",
      "Epoch 105: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7746 - loss: 0.6184 - val_accuracy: 0.9621 - val_loss: 0.2698\n",
      "Epoch 106/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7500 - loss: 0.6158\n",
      "Epoch 106: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7622 - loss: 0.6341 - val_accuracy: 0.9613 - val_loss: 0.2673\n",
      "Epoch 107/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7578 - loss: 0.6896\n",
      "Epoch 107: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7612 - loss: 0.6328 - val_accuracy: 0.9647 - val_loss: 0.2683\n",
      "Epoch 108/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7344 - loss: 0.6880\n",
      "Epoch 108: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7664 - loss: 0.6137 - val_accuracy: 0.9578 - val_loss: 0.2737\n",
      "Epoch 109/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.8047 - loss: 0.5546\n",
      "Epoch 109: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7800 - loss: 0.6060 - val_accuracy: 0.9639 - val_loss: 0.2695\n",
      "Epoch 110/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7578 - loss: 0.6572\n",
      "Epoch 110: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7739 - loss: 0.6084 - val_accuracy: 0.9621 - val_loss: 0.2712\n",
      "Epoch 111/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7422 - loss: 0.6870\n",
      "Epoch 111: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7708 - loss: 0.6112 - val_accuracy: 0.9630 - val_loss: 0.2675\n",
      "Epoch 112/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7891 - loss: 0.5765\n",
      "Epoch 112: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7796 - loss: 0.6045 - val_accuracy: 0.9682 - val_loss: 0.2662\n",
      "Epoch 113/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7266 - loss: 0.7352\n",
      "Epoch 113: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7669 - loss: 0.6301 - val_accuracy: 0.9647 - val_loss: 0.2699\n",
      "Epoch 114/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7578 - loss: 0.6066\n",
      "Epoch 114: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7742 - loss: 0.5984 - val_accuracy: 0.9742 - val_loss: 0.2706\n",
      "Epoch 115/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.8203 - loss: 0.5582\n",
      "Epoch 115: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7935 - loss: 0.5709 - val_accuracy: 0.9664 - val_loss: 0.2617\n",
      "Epoch 116/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7812 - loss: 0.5494\n",
      "Epoch 116: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7780 - loss: 0.6019 - val_accuracy: 0.9690 - val_loss: 0.2586\n",
      "Epoch 117/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.7578 - loss: 0.6459\n",
      "Epoch 117: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7729 - loss: 0.6118 - val_accuracy: 0.9673 - val_loss: 0.2609\n",
      "Epoch 118/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7266 - loss: 0.6792\n",
      "Epoch 118: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7579 - loss: 0.6410 - val_accuracy: 0.9630 - val_loss: 0.2687\n",
      "Epoch 119/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.7656 - loss: 0.5031\n",
      "Epoch 119: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7642 - loss: 0.6140 - val_accuracy: 0.9621 - val_loss: 0.2637\n",
      "Epoch 120/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7266 - loss: 0.7073\n",
      "Epoch 120: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7855 - loss: 0.5824 - val_accuracy: 0.9673 - val_loss: 0.2617\n",
      "Epoch 121/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7734 - loss: 0.5905\n",
      "Epoch 121: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7787 - loss: 0.6035 - val_accuracy: 0.9656 - val_loss: 0.2524\n",
      "Epoch 122/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7422 - loss: 0.6116\n",
      "Epoch 122: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7664 - loss: 0.6061 - val_accuracy: 0.9664 - val_loss: 0.2529\n",
      "Epoch 123/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.8047 - loss: 0.5779\n",
      "Epoch 123: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7795 - loss: 0.6039 - val_accuracy: 0.9750 - val_loss: 0.2556\n",
      "Epoch 124/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7734 - loss: 0.6194\n",
      "Epoch 124: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7759 - loss: 0.5879 - val_accuracy: 0.9682 - val_loss: 0.2531\n",
      "Epoch 125/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7891 - loss: 0.5162\n",
      "Epoch 125: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7872 - loss: 0.5794 - val_accuracy: 0.9733 - val_loss: 0.2561\n",
      "Epoch 126/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7812 - loss: 0.5067\n",
      "Epoch 126: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7962 - loss: 0.5688 - val_accuracy: 0.9707 - val_loss: 0.2476\n",
      "Epoch 127/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.8047 - loss: 0.4736\n",
      "Epoch 127: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7693 - loss: 0.5868 - val_accuracy: 0.9733 - val_loss: 0.2491\n",
      "Epoch 128/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7812 - loss: 0.5966\n",
      "Epoch 128: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7805 - loss: 0.6043 - val_accuracy: 0.9733 - val_loss: 0.2537\n",
      "Epoch 129/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.8359 - loss: 0.5287\n",
      "Epoch 129: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7964 - loss: 0.5617 - val_accuracy: 0.9759 - val_loss: 0.2474\n",
      "Epoch 130/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.8750 - loss: 0.4464\n",
      "Epoch 130: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.8063 - loss: 0.5574 - val_accuracy: 0.9733 - val_loss: 0.2344\n",
      "Epoch 131/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7969 - loss: 0.5644\n",
      "Epoch 131: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7871 - loss: 0.5726 - val_accuracy: 0.9742 - val_loss: 0.2407\n",
      "Epoch 132/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.6953 - loss: 0.7481\n",
      "Epoch 132: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7706 - loss: 0.6085 - val_accuracy: 0.9725 - val_loss: 0.2520\n",
      "Epoch 133/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7578 - loss: 0.6536\n",
      "Epoch 133: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7851 - loss: 0.5934 - val_accuracy: 0.9682 - val_loss: 0.2479\n",
      "Epoch 134/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.8125 - loss: 0.5989\n",
      "Epoch 134: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7899 - loss: 0.5783 - val_accuracy: 0.9716 - val_loss: 0.2437\n",
      "Epoch 135/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.8125 - loss: 0.5642\n",
      "Epoch 135: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7943 - loss: 0.5603 - val_accuracy: 0.9690 - val_loss: 0.2442\n",
      "Epoch 136/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7734 - loss: 0.5905\n",
      "Epoch 136: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7894 - loss: 0.5748 - val_accuracy: 0.9768 - val_loss: 0.2409\n",
      "Epoch 137/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 19ms/step - accuracy: 0.7891 - loss: 0.5676\n",
      "Epoch 137: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7872 - loss: 0.5815 - val_accuracy: 0.9768 - val_loss: 0.2416\n",
      "Epoch 138/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.7812 - loss: 0.6264\n",
      "Epoch 138: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7693 - loss: 0.6066 - val_accuracy: 0.9793 - val_loss: 0.2394\n",
      "Epoch 139/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.8203 - loss: 0.5130\n",
      "Epoch 139: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7814 - loss: 0.5888 - val_accuracy: 0.9742 - val_loss: 0.2390\n",
      "Epoch 140/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7812 - loss: 0.5216\n",
      "Epoch 140: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7843 - loss: 0.5856 - val_accuracy: 0.9742 - val_loss: 0.2312\n",
      "Epoch 141/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7500 - loss: 0.7443\n",
      "Epoch 141: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7798 - loss: 0.5840 - val_accuracy: 0.9742 - val_loss: 0.2284\n",
      "Epoch 142/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.8047 - loss: 0.5053\n",
      "Epoch 142: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7863 - loss: 0.5702 - val_accuracy: 0.9759 - val_loss: 0.2264\n",
      "Epoch 143/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7266 - loss: 0.7747\n",
      "Epoch 143: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7817 - loss: 0.6156 - val_accuracy: 0.9699 - val_loss: 0.2301\n",
      "Epoch 144/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.8125 - loss: 0.5390\n",
      "Epoch 144: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7910 - loss: 0.5857 - val_accuracy: 0.9759 - val_loss: 0.2341\n",
      "Epoch 145/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 20ms/step - accuracy: 0.7422 - loss: 0.6955\n",
      "Epoch 145: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7698 - loss: 0.6111 - val_accuracy: 0.9776 - val_loss: 0.2376\n",
      "Epoch 146/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7656 - loss: 0.5849\n",
      "Epoch 146: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7795 - loss: 0.5752 - val_accuracy: 0.9785 - val_loss: 0.2346\n",
      "Epoch 147/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.8047 - loss: 0.5535\n",
      "Epoch 147: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7970 - loss: 0.5795 - val_accuracy: 0.9819 - val_loss: 0.2286\n",
      "Epoch 148/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.8594 - loss: 0.5247\n",
      "Epoch 148: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7994 - loss: 0.5534 - val_accuracy: 0.9768 - val_loss: 0.2307\n",
      "Epoch 149/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7891 - loss: 0.5924\n",
      "Epoch 149: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7866 - loss: 0.5956 - val_accuracy: 0.9768 - val_loss: 0.2310\n",
      "Epoch 150/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7422 - loss: 0.6836\n",
      "Epoch 150: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7946 - loss: 0.5704 - val_accuracy: 0.9725 - val_loss: 0.2346\n",
      "Epoch 151/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7812 - loss: 0.6361\n",
      "Epoch 151: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7926 - loss: 0.5672 - val_accuracy: 0.9725 - val_loss: 0.2309\n",
      "Epoch 152/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.8203 - loss: 0.5591\n",
      "Epoch 152: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7965 - loss: 0.5665 - val_accuracy: 0.9742 - val_loss: 0.2275\n",
      "Epoch 153/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7344 - loss: 0.6897\n",
      "Epoch 153: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7782 - loss: 0.6005 - val_accuracy: 0.9759 - val_loss: 0.2277\n",
      "Epoch 154/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7422 - loss: 0.6317\n",
      "Epoch 154: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7835 - loss: 0.5750 - val_accuracy: 0.9768 - val_loss: 0.2345\n",
      "Epoch 155/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.8438 - loss: 0.4546\n",
      "Epoch 155: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.8016 - loss: 0.5641 - val_accuracy: 0.9750 - val_loss: 0.2321\n",
      "Epoch 156/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7422 - loss: 0.6906\n",
      "Epoch 156: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7791 - loss: 0.6128 - val_accuracy: 0.9725 - val_loss: 0.2265\n",
      "Epoch 157/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.8359 - loss: 0.4455\n",
      "Epoch 157: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7949 - loss: 0.5442 - val_accuracy: 0.9742 - val_loss: 0.2297\n",
      "Epoch 158/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.7578 - loss: 0.6549\n",
      "Epoch 158: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7914 - loss: 0.5645 - val_accuracy: 0.9768 - val_loss: 0.2274\n",
      "Epoch 159/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7656 - loss: 0.6366\n",
      "Epoch 159: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7872 - loss: 0.5927 - val_accuracy: 0.9802 - val_loss: 0.2305\n",
      "Epoch 160/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7734 - loss: 0.6118\n",
      "Epoch 160: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7822 - loss: 0.5880 - val_accuracy: 0.9785 - val_loss: 0.2348\n",
      "Epoch 161/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7969 - loss: 0.5556\n",
      "Epoch 161: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7844 - loss: 0.5751 - val_accuracy: 0.9785 - val_loss: 0.2264\n",
      "Epoch 162/1000\n",
      "\u001B[1m 1/28\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.8047 - loss: 0.5790\n",
      "Epoch 162: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7942 - loss: 0.5536 - val_accuracy: 0.9776 - val_loss: 0.2327\n",
      "Epoch 162: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x20d79eb4bb0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T01:32:49.335407Z",
     "start_time": "2025-04-07T01:32:49.268888Z"
    }
   },
   "source": [
    "# モデル評価\n",
    "val_loss, val_acc = model.evaluate(X_test, y_test, batch_size=128)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m10/10\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.9737 - loss: 0.2289 \n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T01:32:49.414025Z",
     "start_time": "2025-04-07T01:32:49.351409Z"
    }
   },
   "source": [
    "# 保存したモデルのロード\n",
    "model = tf.keras.models.load_model(model_save_path)"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T01:32:49.493678Z",
     "start_time": "2025-04-07T01:32:49.431099Z"
    }
   },
   "source": [
    "# 推論テスト\n",
    "predict_result = model.predict(np.array([X_test[0]]))\n",
    "print(np.squeeze(predict_result))\n",
    "print(np.argmax(np.squeeze(predict_result)))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 30ms/step\n",
      "[9.8055995e-01 1.9246580e-02 1.6972909e-04 1.5124833e-07 1.4379638e-05\n",
      " 9.2182872e-06]\n",
      "0\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 混同行列"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T01:32:49.746395Z",
     "start_time": "2025-04-07T01:32:49.510717Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "def print_confusion_matrix(y_true, y_pred, report=True):\n",
    "    labels = sorted(list(set(y_true)))\n",
    "    cmx_data = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    \n",
    "    df_cmx = pd.DataFrame(cmx_data, index=labels, columns=labels)\n",
    " \n",
    "    fig, ax = plt.subplots(figsize=(7, 6))\n",
    "    sns.heatmap(df_cmx, annot=True, fmt='g' ,square=False)\n",
    "    ax.set_ylim(len(set(y_true)), 0)\n",
    "    plt.show()\n",
    "    \n",
    "    if report:\n",
    "        print('Classification Report')\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "Y_pred = model.predict(X_test)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "print_confusion_matrix(y_test, y_pred)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m37/37\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 630us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 700x600 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       394\n",
      "           1       1.00      0.95      0.97       238\n",
      "           2       0.96      0.99      0.97       333\n",
      "           3       0.99      0.94      0.96        77\n",
      "           4       1.00      0.97      0.99        71\n",
      "           5       0.94      0.98      0.96        49\n",
      "\n",
      "    accuracy                           0.98      1162\n",
      "   macro avg       0.98      0.97      0.97      1162\n",
      "weighted avg       0.98      0.98      0.98      1162\n",
      "\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow-Lite用のモデルへ変換"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T01:32:49.982375Z",
     "start_time": "2025-04-07T01:32:49.763399Z"
    }
   },
   "source": [
    "# モデルを変換(量子化)\n",
    "tflite_save_path = 'model/keypoint_classifier/keypoint_classifier.tflite'\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_quantized_model = converter.convert()\n",
    "\n",
    "open(tflite_save_path, 'wb').write(tflite_quantized_model)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\grimm\\AppData\\Local\\Temp\\tmp0n5lulxf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\grimm\\AppData\\Local\\Temp\\tmp0n5lulxf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\grimm\\AppData\\Local\\Temp\\tmp0n5lulxf'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 42), dtype=tf.float32, name='input_layer')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 6), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2256939976832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2256939988096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2256939987568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2256940372336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2256940377968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2256940368992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6644"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 推論テスト"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T01:32:50.013297Z",
     "start_time": "2025-04-07T01:32:49.998788Z"
    }
   },
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=tflite_save_path)\n",
    "interpreter.allocate_tensors()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\.venv\\lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
      "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T01:32:50.044788Z",
     "start_time": "2025-04-07T01:32:50.030297Z"
    }
   },
   "source": [
    "# 入出力テンソルを取得\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T01:32:50.075650Z",
     "start_time": "2025-04-07T01:32:50.061303Z"
    }
   },
   "source": [
    "interpreter.set_tensor(input_details[0]['index'], np.array([X_test[0]]))"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-04-07T01:32:50.107549Z",
     "start_time": "2025-04-07T01:32:50.092544Z"
    }
   },
   "source": [
    "%%time\n",
    "# 推論実施\n",
    "interpreter.invoke()\n",
    "tflite_results = interpreter.get_tensor(output_details[0]['index'])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T01:32:50.154657Z",
     "start_time": "2025-04-07T01:32:50.124490Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 推論専用のモデルとして保存\n",
    "model.save(model_save_path, include_optimizer=False)"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T01:32:50.186660Z",
     "start_time": "2025-04-07T01:32:50.171660Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(np.squeeze(tflite_results))\n",
    "print(np.argmax(np.squeeze(tflite_results)))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.8055995e-01 1.9246584e-02 1.6972909e-04 1.5124817e-07 1.4379638e-05\n",
      " 9.2182863e-06]\n",
      "0\n"
     ]
    }
   ],
   "execution_count": 22
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
