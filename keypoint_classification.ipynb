{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T17:56:34.918110Z",
     "start_time": "2025-04-07T17:56:34.913087Z"
    }
   },
   "source": [
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "RANDOM_SEED = 42"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 各パス指定"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T17:56:34.940872Z",
     "start_time": "2025-04-07T17:56:34.934992Z"
    }
   },
   "source": [
    "dataset = 'model/keypoint_classifier/keypoint.csv'\n",
    "\n",
    "# Define the path where the model checkpoint will be saved (with proper path format)\n",
    "model_save_path = r'grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras'\n",
    "\n",
    "# モデルチェックポイントのコールバック\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    model_save_path, verbose=1, save_weights_only=False)\n",
    "\n",
    "# 早期打ち切り用コールバック\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(patience=20, verbose=1)\n",
    "\n",
    "# You can now use these callbacks during model training:\n",
    "# model.fit(X_train, y_train, epochs=100, callbacks=[cp_callback, es_callback])\n"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分類数設定"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T17:56:34.965501Z",
     "start_time": "2025-04-07T17:56:34.960637Z"
    }
   },
   "source": "NUM_CLASSES = 6",
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習データ読み込み"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T17:56:35.108463Z",
     "start_time": "2025-04-07T17:56:34.985362Z"
    }
   },
   "source": [
    "X_dataset = np.loadtxt(dataset, delimiter=',', dtype='float32', usecols=list(range(1, (21 * 2) + 1)))"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T17:56:35.166526Z",
     "start_time": "2025-04-07T17:56:35.123706Z"
    }
   },
   "source": "y_dataset = np.loadtxt(dataset, delimiter=',', dtype='int32', usecols=(0))",
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T17:56:35.188062Z",
     "start_time": "2025-04-07T17:56:35.182035Z"
    }
   },
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_dataset, y_dataset, train_size=0.75, random_state=RANDOM_SEED)"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデル構築"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T17:56:35.235997Z",
     "start_time": "2025-04-07T17:56:35.205160Z"
    }
   },
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input((21 * 2, )),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(20, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(10, activation='relu'),\n",
    "    tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "])"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T17:56:35.272049Z",
     "start_time": "2025-04-07T17:56:35.252374Z"
    }
   },
   "source": [
    "model.summary()  # tf.keras.utils.plot_model(model, show_shapes=True)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"sequential_1\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dropout_2 (\u001B[38;5;33mDropout\u001B[0m)             │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m42\u001B[0m)             │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m20\u001B[0m)             │           \u001B[38;5;34m860\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001B[38;5;33mDropout\u001B[0m)             │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m20\u001B[0m)             │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m10\u001B[0m)             │           \u001B[38;5;34m210\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m6\u001B[0m)              │            \u001B[38;5;34m66\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">860</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">210</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m1,136\u001B[0m (4.44 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,136</span> (4.44 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m1,136\u001B[0m (4.44 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,136</span> (4.44 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T17:56:35.347833Z",
     "start_time": "2025-04-07T17:56:35.342268Z"
    }
   },
   "source": [
    "# モデルチェックポイントのコールバック\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    model_save_path, verbose=1, save_weights_only=False)\n",
    "# 早期打ち切り用コールバック\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(patience=20, verbose=1)"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T17:56:35.374930Z",
     "start_time": "2025-04-07T17:56:35.365805Z"
    }
   },
   "source": [
    "# モデルコンパイル\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデル訓練"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-04-07T17:57:21.162468Z",
     "start_time": "2025-04-07T17:56:35.392416Z"
    }
   },
   "source": [
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=1000,\n",
    "    batch_size=128,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[cp_callback, es_callback]\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "\u001B[1m24/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.1545 - loss: 1.8280\n",
      "Epoch 1: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 14ms/step - accuracy: 0.1586 - loss: 1.8212 - val_accuracy: 0.2580 - val_loss: 1.6910\n",
      "Epoch 2/1000\n",
      "\u001B[1m26/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.2312 - loss: 1.7057 \n",
      "Epoch 2: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.2335 - loss: 1.7043 - val_accuracy: 0.2939 - val_loss: 1.6295\n",
      "Epoch 3/1000\n",
      "\u001B[1m24/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.2898 - loss: 1.6337 \n",
      "Epoch 3: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.2915 - loss: 1.6344 - val_accuracy: 0.3265 - val_loss: 1.5792\n",
      "Epoch 4/1000\n",
      "\u001B[1m26/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.3366 - loss: 1.5895 \n",
      "Epoch 4: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.3361 - loss: 1.5893 - val_accuracy: 0.3633 - val_loss: 1.5268\n",
      "Epoch 5/1000\n",
      "\u001B[1m25/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.3660 - loss: 1.5450 \n",
      "Epoch 5: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.3655 - loss: 1.5433 - val_accuracy: 0.4229 - val_loss: 1.4700\n",
      "Epoch 6/1000\n",
      "\u001B[1m25/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.3819 - loss: 1.4869 \n",
      "Epoch 6: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.3826 - loss: 1.4859 - val_accuracy: 0.4865 - val_loss: 1.3971\n",
      "Epoch 7/1000\n",
      "\u001B[1m24/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.3936 - loss: 1.4433 \n",
      "Epoch 7: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.3964 - loss: 1.4420 - val_accuracy: 0.5224 - val_loss: 1.3248\n",
      "Epoch 8/1000\n",
      "\u001B[1m25/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.4212 - loss: 1.4074 \n",
      "Epoch 8: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.4231 - loss: 1.4040 - val_accuracy: 0.5404 - val_loss: 1.2685\n",
      "Epoch 9/1000\n",
      "\u001B[1m26/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.4534 - loss: 1.3447 \n",
      "Epoch 9: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.4535 - loss: 1.3449 - val_accuracy: 0.5706 - val_loss: 1.2141\n",
      "Epoch 10/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.4577 - loss: 1.3313 \n",
      "Epoch 10: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.4588 - loss: 1.3293 - val_accuracy: 0.5780 - val_loss: 1.1640\n",
      "Epoch 11/1000\n",
      "\u001B[1m26/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.4818 - loss: 1.2951 \n",
      "Epoch 11: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.4827 - loss: 1.2930 - val_accuracy: 0.6114 - val_loss: 1.1138\n",
      "Epoch 12/1000\n",
      "\u001B[1m28/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.4951 - loss: 1.2396 \n",
      "Epoch 12: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.4950 - loss: 1.2397 - val_accuracy: 0.6229 - val_loss: 1.0694\n",
      "Epoch 13/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.5028 - loss: 1.2274 \n",
      "Epoch 13: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.5030 - loss: 1.2269 - val_accuracy: 0.6490 - val_loss: 1.0298\n",
      "Epoch 14/1000\n",
      "\u001B[1m25/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.5157 - loss: 1.1883 \n",
      "Epoch 14: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.5157 - loss: 1.1878 - val_accuracy: 0.6604 - val_loss: 0.9878\n",
      "Epoch 15/1000\n",
      "\u001B[1m24/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.5310 - loss: 1.1773 \n",
      "Epoch 15: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.5284 - loss: 1.1784 - val_accuracy: 0.6743 - val_loss: 0.9587\n",
      "Epoch 16/1000\n",
      "\u001B[1m25/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.5402 - loss: 1.1466 \n",
      "Epoch 16: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.5393 - loss: 1.1474 - val_accuracy: 0.6824 - val_loss: 0.9221\n",
      "Epoch 17/1000\n",
      "\u001B[1m26/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.5629 - loss: 1.1219 \n",
      "Epoch 17: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.5608 - loss: 1.1235 - val_accuracy: 0.6955 - val_loss: 0.8929\n",
      "Epoch 18/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.5311 - loss: 1.1320 \n",
      "Epoch 18: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.5330 - loss: 1.1299 - val_accuracy: 0.7012 - val_loss: 0.8629\n",
      "Epoch 19/1000\n",
      "\u001B[1m26/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.5710 - loss: 1.0724 \n",
      "Epoch 19: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.5701 - loss: 1.0731 - val_accuracy: 0.7200 - val_loss: 0.8331\n",
      "Epoch 20/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.5755 - loss: 1.0733 \n",
      "Epoch 20: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.5745 - loss: 1.0737 - val_accuracy: 0.7355 - val_loss: 0.8115\n",
      "Epoch 21/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.5659 - loss: 1.0915 \n",
      "Epoch 21: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.5676 - loss: 1.0880 - val_accuracy: 0.7469 - val_loss: 0.7906\n",
      "Epoch 22/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.5792 - loss: 1.0447 \n",
      "Epoch 22: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.5797 - loss: 1.0450 - val_accuracy: 0.7739 - val_loss: 0.7632\n",
      "Epoch 23/1000\n",
      "\u001B[1m26/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.5841 - loss: 1.0273 \n",
      "Epoch 23: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.5855 - loss: 1.0274 - val_accuracy: 0.7780 - val_loss: 0.7446\n",
      "Epoch 24/1000\n",
      "\u001B[1m25/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.5933 - loss: 1.0346 \n",
      "Epoch 24: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.5942 - loss: 1.0318 - val_accuracy: 0.7918 - val_loss: 0.7274\n",
      "Epoch 25/1000\n",
      "\u001B[1m26/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6105 - loss: 1.0004 \n",
      "Epoch 25: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.6111 - loss: 1.0001 - val_accuracy: 0.7951 - val_loss: 0.7083\n",
      "Epoch 26/1000\n",
      "\u001B[1m23/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.5945 - loss: 1.0041 \n",
      "Epoch 26: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.5975 - loss: 1.0012 - val_accuracy: 0.8155 - val_loss: 0.6875\n",
      "Epoch 27/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6253 - loss: 0.9577 \n",
      "Epoch 27: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.6242 - loss: 0.9586 - val_accuracy: 0.8253 - val_loss: 0.6712\n",
      "Epoch 28/1000\n",
      "\u001B[1m26/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6219 - loss: 0.9526 \n",
      "Epoch 28: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.6216 - loss: 0.9532 - val_accuracy: 0.8310 - val_loss: 0.6568\n",
      "Epoch 29/1000\n",
      "\u001B[1m25/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6327 - loss: 0.9437 \n",
      "Epoch 29: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.6314 - loss: 0.9460 - val_accuracy: 0.8457 - val_loss: 0.6389\n",
      "Epoch 30/1000\n",
      "\u001B[1m26/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6400 - loss: 0.9174 \n",
      "Epoch 30: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.6405 - loss: 0.9183 - val_accuracy: 0.8465 - val_loss: 0.6213\n",
      "Epoch 31/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6462 - loss: 0.9191 \n",
      "Epoch 31: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.6463 - loss: 0.9185 - val_accuracy: 0.8555 - val_loss: 0.6079\n",
      "Epoch 32/1000\n",
      "\u001B[1m26/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6455 - loss: 0.9068 \n",
      "Epoch 32: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.6461 - loss: 0.9051 - val_accuracy: 0.8531 - val_loss: 0.5833\n",
      "Epoch 33/1000\n",
      "\u001B[1m26/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6459 - loss: 0.9071 \n",
      "Epoch 33: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.6464 - loss: 0.9056 - val_accuracy: 0.8702 - val_loss: 0.5713\n",
      "Epoch 34/1000\n",
      "\u001B[1m26/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6612 - loss: 0.8746 \n",
      "Epoch 34: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.6607 - loss: 0.8751 - val_accuracy: 0.8718 - val_loss: 0.5549\n",
      "Epoch 35/1000\n",
      "\u001B[1m26/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6613 - loss: 0.8699 \n",
      "Epoch 35: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.6606 - loss: 0.8730 - val_accuracy: 0.8808 - val_loss: 0.5527\n",
      "Epoch 36/1000\n",
      "\u001B[1m26/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6724 - loss: 0.8660 \n",
      "Epoch 36: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.6722 - loss: 0.8656 - val_accuracy: 0.8816 - val_loss: 0.5398\n",
      "Epoch 37/1000\n",
      "\u001B[1m26/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6692 - loss: 0.8524 \n",
      "Epoch 37: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.6676 - loss: 0.8530 - val_accuracy: 0.8645 - val_loss: 0.5234\n",
      "Epoch 38/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6721 - loss: 0.8518 \n",
      "Epoch 38: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.6717 - loss: 0.8530 - val_accuracy: 0.8971 - val_loss: 0.5196\n",
      "Epoch 39/1000\n",
      "\u001B[1m25/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6649 - loss: 0.8795 \n",
      "Epoch 39: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.6668 - loss: 0.8765 - val_accuracy: 0.8784 - val_loss: 0.5165\n",
      "Epoch 40/1000\n",
      "\u001B[1m26/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6584 - loss: 0.8661 \n",
      "Epoch 40: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.6591 - loss: 0.8661 - val_accuracy: 0.9020 - val_loss: 0.4999\n",
      "Epoch 41/1000\n",
      "\u001B[1m26/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6720 - loss: 0.8531 \n",
      "Epoch 41: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.6727 - loss: 0.8517 - val_accuracy: 0.8906 - val_loss: 0.4965\n",
      "Epoch 42/1000\n",
      "\u001B[1m26/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6764 - loss: 0.8409 \n",
      "Epoch 42: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.6767 - loss: 0.8401 - val_accuracy: 0.9069 - val_loss: 0.4889\n",
      "Epoch 43/1000\n",
      "\u001B[1m25/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6827 - loss: 0.8412 \n",
      "Epoch 43: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.6838 - loss: 0.8363 - val_accuracy: 0.9061 - val_loss: 0.4752\n",
      "Epoch 44/1000\n",
      "\u001B[1m26/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7025 - loss: 0.7878 \n",
      "Epoch 44: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7014 - loss: 0.7899 - val_accuracy: 0.9110 - val_loss: 0.4647\n",
      "Epoch 45/1000\n",
      "\u001B[1m26/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6738 - loss: 0.8286 \n",
      "Epoch 45: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.6743 - loss: 0.8285 - val_accuracy: 0.9086 - val_loss: 0.4648\n",
      "Epoch 46/1000\n",
      "\u001B[1m26/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6883 - loss: 0.8157 \n",
      "Epoch 46: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.6884 - loss: 0.8159 - val_accuracy: 0.9192 - val_loss: 0.4536\n",
      "Epoch 47/1000\n",
      "\u001B[1m25/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6945 - loss: 0.8019 \n",
      "Epoch 47: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.6942 - loss: 0.8023 - val_accuracy: 0.9249 - val_loss: 0.4565\n",
      "Epoch 48/1000\n",
      "\u001B[1m26/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6956 - loss: 0.7978 \n",
      "Epoch 48: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.6967 - loss: 0.7980 - val_accuracy: 0.9208 - val_loss: 0.4462\n",
      "Epoch 49/1000\n",
      "\u001B[1m26/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7063 - loss: 0.7912 \n",
      "Epoch 49: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7046 - loss: 0.7938 - val_accuracy: 0.9200 - val_loss: 0.4388\n",
      "Epoch 50/1000\n",
      "\u001B[1m25/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6902 - loss: 0.8052 \n",
      "Epoch 50: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.6893 - loss: 0.8059 - val_accuracy: 0.9233 - val_loss: 0.4395\n",
      "Epoch 51/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6866 - loss: 0.7924 \n",
      "Epoch 51: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.6877 - loss: 0.7917 - val_accuracy: 0.9298 - val_loss: 0.4324\n",
      "Epoch 52/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7188 - loss: 0.7508 \n",
      "Epoch 52: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7180 - loss: 0.7536 - val_accuracy: 0.9388 - val_loss: 0.4281\n",
      "Epoch 53/1000\n",
      "\u001B[1m26/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7041 - loss: 0.8016 \n",
      "Epoch 53: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7051 - loss: 0.7996 - val_accuracy: 0.9322 - val_loss: 0.4196\n",
      "Epoch 54/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7087 - loss: 0.7893 \n",
      "Epoch 54: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7092 - loss: 0.7881 - val_accuracy: 0.9355 - val_loss: 0.4167\n",
      "Epoch 55/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7138 - loss: 0.7738 \n",
      "Epoch 55: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7131 - loss: 0.7748 - val_accuracy: 0.9396 - val_loss: 0.4096\n",
      "Epoch 56/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7044 - loss: 0.7897 \n",
      "Epoch 56: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7052 - loss: 0.7886 - val_accuracy: 0.9429 - val_loss: 0.4038\n",
      "Epoch 57/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7153 - loss: 0.7565 \n",
      "Epoch 57: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7159 - loss: 0.7557 - val_accuracy: 0.9453 - val_loss: 0.4012\n",
      "Epoch 58/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7400 - loss: 0.7305 \n",
      "Epoch 58: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7387 - loss: 0.7333 - val_accuracy: 0.9469 - val_loss: 0.3995\n",
      "Epoch 59/1000\n",
      "\u001B[1m26/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7279 - loss: 0.7470 \n",
      "Epoch 59: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7273 - loss: 0.7490 - val_accuracy: 0.9461 - val_loss: 0.3952\n",
      "Epoch 60/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7084 - loss: 0.7781 \n",
      "Epoch 60: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7090 - loss: 0.7768 - val_accuracy: 0.9445 - val_loss: 0.3917\n",
      "Epoch 61/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7139 - loss: 0.7597 \n",
      "Epoch 61: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7139 - loss: 0.7597 - val_accuracy: 0.9445 - val_loss: 0.3866\n",
      "Epoch 62/1000\n",
      "\u001B[1m26/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7336 - loss: 0.7462 \n",
      "Epoch 62: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7332 - loss: 0.7472 - val_accuracy: 0.9453 - val_loss: 0.3796\n",
      "Epoch 63/1000\n",
      "\u001B[1m26/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7362 - loss: 0.7243 \n",
      "Epoch 63: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7346 - loss: 0.7271 - val_accuracy: 0.9478 - val_loss: 0.3736\n",
      "Epoch 64/1000\n",
      "\u001B[1m26/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7119 - loss: 0.7666 \n",
      "Epoch 64: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7134 - loss: 0.7644 - val_accuracy: 0.9461 - val_loss: 0.3751\n",
      "Epoch 65/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7314 - loss: 0.7137 \n",
      "Epoch 65: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7311 - loss: 0.7155 - val_accuracy: 0.9469 - val_loss: 0.3666\n",
      "Epoch 66/1000\n",
      "\u001B[1m26/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7182 - loss: 0.7840 \n",
      "Epoch 66: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7200 - loss: 0.7793 - val_accuracy: 0.9469 - val_loss: 0.3751\n",
      "Epoch 67/1000\n",
      "\u001B[1m28/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7347 - loss: 0.7426 \n",
      "Epoch 67: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.7342 - loss: 0.7435 - val_accuracy: 0.9502 - val_loss: 0.3662\n",
      "Epoch 68/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7268 - loss: 0.7548 \n",
      "Epoch 68: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7271 - loss: 0.7535 - val_accuracy: 0.9445 - val_loss: 0.3704\n",
      "Epoch 69/1000\n",
      "\u001B[1m26/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7211 - loss: 0.7350 \n",
      "Epoch 69: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7221 - loss: 0.7351 - val_accuracy: 0.9502 - val_loss: 0.3646\n",
      "Epoch 70/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7333 - loss: 0.7237 \n",
      "Epoch 70: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7328 - loss: 0.7256 - val_accuracy: 0.9469 - val_loss: 0.3663\n",
      "Epoch 71/1000\n",
      "\u001B[1m28/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7157 - loss: 0.7360 \n",
      "Epoch 71: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7165 - loss: 0.7356 - val_accuracy: 0.9478 - val_loss: 0.3690\n",
      "Epoch 72/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7402 - loss: 0.7267 \n",
      "Epoch 72: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7400 - loss: 0.7272 - val_accuracy: 0.9486 - val_loss: 0.3596\n",
      "Epoch 73/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7395 - loss: 0.7158 \n",
      "Epoch 73: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7396 - loss: 0.7166 - val_accuracy: 0.9461 - val_loss: 0.3589\n",
      "Epoch 74/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7369 - loss: 0.7416 \n",
      "Epoch 74: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7371 - loss: 0.7415 - val_accuracy: 0.9486 - val_loss: 0.3546\n",
      "Epoch 75/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7345 - loss: 0.7278 \n",
      "Epoch 75: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7341 - loss: 0.7282 - val_accuracy: 0.9486 - val_loss: 0.3587\n",
      "Epoch 76/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7261 - loss: 0.7420 \n",
      "Epoch 76: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7259 - loss: 0.7426 - val_accuracy: 0.9478 - val_loss: 0.3620\n",
      "Epoch 77/1000\n",
      "\u001B[1m25/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7275 - loss: 0.7524 \n",
      "Epoch 77: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7280 - loss: 0.7498 - val_accuracy: 0.9502 - val_loss: 0.3529\n",
      "Epoch 78/1000\n",
      "\u001B[1m26/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7486 - loss: 0.7156 \n",
      "Epoch 78: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7472 - loss: 0.7183 - val_accuracy: 0.9445 - val_loss: 0.3564\n",
      "Epoch 79/1000\n",
      "\u001B[1m26/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7309 - loss: 0.7128 \n",
      "Epoch 79: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7315 - loss: 0.7143 - val_accuracy: 0.9478 - val_loss: 0.3503\n",
      "Epoch 80/1000\n",
      "\u001B[1m26/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7090 - loss: 0.7776 \n",
      "Epoch 80: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7109 - loss: 0.7727 - val_accuracy: 0.9478 - val_loss: 0.3528\n",
      "Epoch 81/1000\n",
      "\u001B[1m26/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7417 - loss: 0.7313 \n",
      "Epoch 81: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7415 - loss: 0.7304 - val_accuracy: 0.9518 - val_loss: 0.3505\n",
      "Epoch 82/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7447 - loss: 0.6943 \n",
      "Epoch 82: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7443 - loss: 0.6964 - val_accuracy: 0.9478 - val_loss: 0.3511\n",
      "Epoch 83/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7601 - loss: 0.6828 \n",
      "Epoch 83: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7588 - loss: 0.6847 - val_accuracy: 0.9478 - val_loss: 0.3534\n",
      "Epoch 84/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7347 - loss: 0.7145 \n",
      "Epoch 84: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7363 - loss: 0.7127 - val_accuracy: 0.9502 - val_loss: 0.3447\n",
      "Epoch 85/1000\n",
      "\u001B[1m26/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7264 - loss: 0.7301 \n",
      "Epoch 85: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7279 - loss: 0.7279 - val_accuracy: 0.9535 - val_loss: 0.3381\n",
      "Epoch 86/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7466 - loss: 0.6826 \n",
      "Epoch 86: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7457 - loss: 0.6848 - val_accuracy: 0.9527 - val_loss: 0.3399\n",
      "Epoch 87/1000\n",
      "\u001B[1m28/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7412 - loss: 0.6976 \n",
      "Epoch 87: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7410 - loss: 0.6988 - val_accuracy: 0.9535 - val_loss: 0.3390\n",
      "Epoch 88/1000\n",
      "\u001B[1m26/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7558 - loss: 0.6974 \n",
      "Epoch 88: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7536 - loss: 0.6994 - val_accuracy: 0.9486 - val_loss: 0.3364\n",
      "Epoch 89/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7549 - loss: 0.6759 \n",
      "Epoch 89: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7538 - loss: 0.6785 - val_accuracy: 0.9494 - val_loss: 0.3350\n",
      "Epoch 90/1000\n",
      "\u001B[1m26/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7353 - loss: 0.7389 \n",
      "Epoch 90: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7358 - loss: 0.7353 - val_accuracy: 0.9502 - val_loss: 0.3348\n",
      "Epoch 91/1000\n",
      "\u001B[1m26/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7356 - loss: 0.7307 \n",
      "Epoch 91: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7360 - loss: 0.7306 - val_accuracy: 0.9502 - val_loss: 0.3427\n",
      "Epoch 92/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7339 - loss: 0.7003 \n",
      "Epoch 92: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7342 - loss: 0.7001 - val_accuracy: 0.9453 - val_loss: 0.3308\n",
      "Epoch 93/1000\n",
      "\u001B[1m26/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7408 - loss: 0.7166 \n",
      "Epoch 93: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7411 - loss: 0.7149 - val_accuracy: 0.9494 - val_loss: 0.3309\n",
      "Epoch 94/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7450 - loss: 0.7122 \n",
      "Epoch 94: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7455 - loss: 0.7102 - val_accuracy: 0.9486 - val_loss: 0.3280\n",
      "Epoch 95/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7365 - loss: 0.7048 \n",
      "Epoch 95: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - accuracy: 0.7372 - loss: 0.7060 - val_accuracy: 0.9437 - val_loss: 0.3322\n",
      "Epoch 96/1000\n",
      "\u001B[1m26/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7291 - loss: 0.7077 \n",
      "Epoch 96: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7299 - loss: 0.7074 - val_accuracy: 0.9469 - val_loss: 0.3330\n",
      "Epoch 97/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7527 - loss: 0.6878 \n",
      "Epoch 97: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7529 - loss: 0.6875 - val_accuracy: 0.9510 - val_loss: 0.3244\n",
      "Epoch 98/1000\n",
      "\u001B[1m28/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7553 - loss: 0.6839 \n",
      "Epoch 98: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7549 - loss: 0.6841 - val_accuracy: 0.9478 - val_loss: 0.3379\n",
      "Epoch 99/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7455 - loss: 0.6893 \n",
      "Epoch 99: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7454 - loss: 0.6904 - val_accuracy: 0.9518 - val_loss: 0.3313\n",
      "Epoch 100/1000\n",
      "\u001B[1m26/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7540 - loss: 0.7027 \n",
      "Epoch 100: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7527 - loss: 0.7047 - val_accuracy: 0.9494 - val_loss: 0.3359\n",
      "Epoch 101/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7391 - loss: 0.7027 \n",
      "Epoch 101: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7396 - loss: 0.7031 - val_accuracy: 0.9527 - val_loss: 0.3335\n",
      "Epoch 102/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7299 - loss: 0.7174 \n",
      "Epoch 102: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - accuracy: 0.7295 - loss: 0.7178 - val_accuracy: 0.9527 - val_loss: 0.3259\n",
      "Epoch 103/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7245 - loss: 0.7228 \n",
      "Epoch 103: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7257 - loss: 0.7210 - val_accuracy: 0.9551 - val_loss: 0.3263\n",
      "Epoch 104/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7540 - loss: 0.6862 \n",
      "Epoch 104: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - accuracy: 0.7545 - loss: 0.6857 - val_accuracy: 0.9592 - val_loss: 0.3168\n",
      "Epoch 105/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7534 - loss: 0.6830 \n",
      "Epoch 105: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7526 - loss: 0.6849 - val_accuracy: 0.9502 - val_loss: 0.3295\n",
      "Epoch 106/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7376 - loss: 0.7248 \n",
      "Epoch 106: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7383 - loss: 0.7220 - val_accuracy: 0.9559 - val_loss: 0.3174\n",
      "Epoch 107/1000\n",
      "\u001B[1m28/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7556 - loss: 0.6776 \n",
      "Epoch 107: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - accuracy: 0.7548 - loss: 0.6792 - val_accuracy: 0.9527 - val_loss: 0.3216\n",
      "Epoch 108/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7728 - loss: 0.6493 \n",
      "Epoch 108: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7724 - loss: 0.6502 - val_accuracy: 0.9494 - val_loss: 0.3214\n",
      "Epoch 109/1000\n",
      "\u001B[1m28/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7343 - loss: 0.7143 \n",
      "Epoch 109: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7349 - loss: 0.7130 - val_accuracy: 0.9518 - val_loss: 0.3197\n",
      "Epoch 110/1000\n",
      "\u001B[1m26/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7674 - loss: 0.6581 \n",
      "Epoch 110: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7665 - loss: 0.6589 - val_accuracy: 0.9445 - val_loss: 0.3231\n",
      "Epoch 111/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7532 - loss: 0.6671 \n",
      "Epoch 111: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - accuracy: 0.7530 - loss: 0.6681 - val_accuracy: 0.9510 - val_loss: 0.3200\n",
      "Epoch 112/1000\n",
      "\u001B[1m28/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7612 - loss: 0.6503 \n",
      "Epoch 112: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7606 - loss: 0.6516 - val_accuracy: 0.9600 - val_loss: 0.3138\n",
      "Epoch 113/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7553 - loss: 0.7198 \n",
      "Epoch 113: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7553 - loss: 0.7170 - val_accuracy: 0.9518 - val_loss: 0.3177\n",
      "Epoch 114/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7561 - loss: 0.6736 \n",
      "Epoch 114: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - accuracy: 0.7562 - loss: 0.6748 - val_accuracy: 0.9584 - val_loss: 0.3167\n",
      "Epoch 115/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7514 - loss: 0.6842 \n",
      "Epoch 115: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7512 - loss: 0.6844 - val_accuracy: 0.9502 - val_loss: 0.3213\n",
      "Epoch 116/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7605 - loss: 0.6664 \n",
      "Epoch 116: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7594 - loss: 0.6672 - val_accuracy: 0.9584 - val_loss: 0.3107\n",
      "Epoch 117/1000\n",
      "\u001B[1m28/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7547 - loss: 0.6749 \n",
      "Epoch 117: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - accuracy: 0.7547 - loss: 0.6748 - val_accuracy: 0.9535 - val_loss: 0.3134\n",
      "Epoch 118/1000\n",
      "\u001B[1m28/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7640 - loss: 0.6624 \n",
      "Epoch 118: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7638 - loss: 0.6625 - val_accuracy: 0.9510 - val_loss: 0.3140\n",
      "Epoch 119/1000\n",
      "\u001B[1m26/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7441 - loss: 0.6912 \n",
      "Epoch 119: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7444 - loss: 0.6920 - val_accuracy: 0.9608 - val_loss: 0.3187\n",
      "Epoch 120/1000\n",
      "\u001B[1m28/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7481 - loss: 0.6813 \n",
      "Epoch 120: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7484 - loss: 0.6806 - val_accuracy: 0.9486 - val_loss: 0.3178\n",
      "Epoch 121/1000\n",
      "\u001B[1m28/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7605 - loss: 0.6765 \n",
      "Epoch 121: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - accuracy: 0.7606 - loss: 0.6756 - val_accuracy: 0.9551 - val_loss: 0.3094\n",
      "Epoch 122/1000\n",
      "\u001B[1m28/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7751 - loss: 0.6550 \n",
      "Epoch 122: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7739 - loss: 0.6562 - val_accuracy: 0.9592 - val_loss: 0.3056\n",
      "Epoch 123/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7879 - loss: 0.6262 \n",
      "Epoch 123: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7864 - loss: 0.6290 - val_accuracy: 0.9494 - val_loss: 0.3165\n",
      "Epoch 124/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7358 - loss: 0.6831 \n",
      "Epoch 124: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7369 - loss: 0.6831 - val_accuracy: 0.9649 - val_loss: 0.3038\n",
      "Epoch 125/1000\n",
      "\u001B[1m28/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7576 - loss: 0.6608 \n",
      "Epoch 125: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - accuracy: 0.7574 - loss: 0.6619 - val_accuracy: 0.9527 - val_loss: 0.3103\n",
      "Epoch 126/1000\n",
      "\u001B[1m28/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7584 - loss: 0.6808 \n",
      "Epoch 126: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7584 - loss: 0.6801 - val_accuracy: 0.9502 - val_loss: 0.3105\n",
      "Epoch 127/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7520 - loss: 0.6661 \n",
      "Epoch 127: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7515 - loss: 0.6685 - val_accuracy: 0.9576 - val_loss: 0.3102\n",
      "Epoch 128/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7655 - loss: 0.6379 \n",
      "Epoch 128: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7651 - loss: 0.6399 - val_accuracy: 0.9600 - val_loss: 0.3059\n",
      "Epoch 129/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7628 - loss: 0.6587 \n",
      "Epoch 129: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7625 - loss: 0.6583 - val_accuracy: 0.9518 - val_loss: 0.3119\n",
      "Epoch 130/1000\n",
      "\u001B[1m26/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7471 - loss: 0.6827 \n",
      "Epoch 130: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7487 - loss: 0.6793 - val_accuracy: 0.9616 - val_loss: 0.3026\n",
      "Epoch 131/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7718 - loss: 0.6472 \n",
      "Epoch 131: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7716 - loss: 0.6473 - val_accuracy: 0.9535 - val_loss: 0.3016\n",
      "Epoch 132/1000\n",
      "\u001B[1m28/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7701 - loss: 0.6323 \n",
      "Epoch 132: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7699 - loss: 0.6332 - val_accuracy: 0.9494 - val_loss: 0.3045\n",
      "Epoch 133/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7449 - loss: 0.6996 \n",
      "Epoch 133: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7463 - loss: 0.6974 - val_accuracy: 0.9616 - val_loss: 0.3027\n",
      "Epoch 134/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7419 - loss: 0.6635 \n",
      "Epoch 134: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7436 - loss: 0.6614 - val_accuracy: 0.9478 - val_loss: 0.3053\n",
      "Epoch 135/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7630 - loss: 0.6435 \n",
      "Epoch 135: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7628 - loss: 0.6446 - val_accuracy: 0.9510 - val_loss: 0.3079\n",
      "Epoch 136/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7612 - loss: 0.6458 \n",
      "Epoch 136: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7611 - loss: 0.6474 - val_accuracy: 0.9649 - val_loss: 0.3033\n",
      "Epoch 137/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7763 - loss: 0.6465 \n",
      "Epoch 137: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7761 - loss: 0.6462 - val_accuracy: 0.9535 - val_loss: 0.3068\n",
      "Epoch 138/1000\n",
      "\u001B[1m28/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7658 - loss: 0.6609 \n",
      "Epoch 138: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - accuracy: 0.7658 - loss: 0.6606 - val_accuracy: 0.9600 - val_loss: 0.2926\n",
      "Epoch 139/1000\n",
      "\u001B[1m28/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7611 - loss: 0.6492 \n",
      "Epoch 139: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - accuracy: 0.7615 - loss: 0.6497 - val_accuracy: 0.9633 - val_loss: 0.2943\n",
      "Epoch 140/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7495 - loss: 0.6822 \n",
      "Epoch 140: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - accuracy: 0.7512 - loss: 0.6806 - val_accuracy: 0.9551 - val_loss: 0.2981\n",
      "Epoch 141/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7668 - loss: 0.6295 \n",
      "Epoch 141: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - accuracy: 0.7667 - loss: 0.6317 - val_accuracy: 0.9551 - val_loss: 0.2990\n",
      "Epoch 142/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7698 - loss: 0.6328 \n",
      "Epoch 142: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7696 - loss: 0.6351 - val_accuracy: 0.9510 - val_loss: 0.3007\n",
      "Epoch 143/1000\n",
      "\u001B[1m28/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7558 - loss: 0.6556 \n",
      "Epoch 143: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7558 - loss: 0.6559 - val_accuracy: 0.9576 - val_loss: 0.2975\n",
      "Epoch 144/1000\n",
      "\u001B[1m28/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7585 - loss: 0.6583 \n",
      "Epoch 144: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7591 - loss: 0.6572 - val_accuracy: 0.9559 - val_loss: 0.2974\n",
      "Epoch 145/1000\n",
      "\u001B[1m28/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7507 - loss: 0.6947 \n",
      "Epoch 145: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7515 - loss: 0.6924 - val_accuracy: 0.9559 - val_loss: 0.3002\n",
      "Epoch 146/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7560 - loss: 0.6621 \n",
      "Epoch 146: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7570 - loss: 0.6605 - val_accuracy: 0.9559 - val_loss: 0.3025\n",
      "Epoch 147/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7575 - loss: 0.6433 \n",
      "Epoch 147: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7578 - loss: 0.6442 - val_accuracy: 0.9494 - val_loss: 0.3038\n",
      "Epoch 148/1000\n",
      "\u001B[1m28/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7773 - loss: 0.6229 \n",
      "Epoch 148: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7773 - loss: 0.6231 - val_accuracy: 0.9567 - val_loss: 0.2959\n",
      "Epoch 149/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7665 - loss: 0.6494 \n",
      "Epoch 149: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - accuracy: 0.7668 - loss: 0.6479 - val_accuracy: 0.9559 - val_loss: 0.3046\n",
      "Epoch 150/1000\n",
      "\u001B[1m28/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7694 - loss: 0.6464 \n",
      "Epoch 150: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - accuracy: 0.7695 - loss: 0.6460 - val_accuracy: 0.9633 - val_loss: 0.2993\n",
      "Epoch 151/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7822 - loss: 0.6011 \n",
      "Epoch 151: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7810 - loss: 0.6035 - val_accuracy: 0.9559 - val_loss: 0.3010\n",
      "Epoch 152/1000\n",
      "\u001B[1m28/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7486 - loss: 0.6870 \n",
      "Epoch 152: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - accuracy: 0.7498 - loss: 0.6850 - val_accuracy: 0.9559 - val_loss: 0.3015\n",
      "Epoch 153/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7736 - loss: 0.6419 \n",
      "Epoch 153: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7738 - loss: 0.6419 - val_accuracy: 0.9510 - val_loss: 0.3034\n",
      "Epoch 154/1000\n",
      "\u001B[1m28/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7915 - loss: 0.6105 \n",
      "Epoch 154: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - accuracy: 0.7904 - loss: 0.6121 - val_accuracy: 0.9624 - val_loss: 0.2917\n",
      "Epoch 155/1000\n",
      "\u001B[1m28/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7807 - loss: 0.6294 \n",
      "Epoch 155: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7806 - loss: 0.6294 - val_accuracy: 0.9592 - val_loss: 0.2956\n",
      "Epoch 156/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7797 - loss: 0.6410 \n",
      "Epoch 156: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - accuracy: 0.7798 - loss: 0.6390 - val_accuracy: 0.9510 - val_loss: 0.2955\n",
      "Epoch 157/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7684 - loss: 0.6467 \n",
      "Epoch 157: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7679 - loss: 0.6469 - val_accuracy: 0.9584 - val_loss: 0.2953\n",
      "Epoch 158/1000\n",
      "\u001B[1m28/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7752 - loss: 0.6344 \n",
      "Epoch 158: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - accuracy: 0.7750 - loss: 0.6339 - val_accuracy: 0.9641 - val_loss: 0.2848\n",
      "Epoch 159/1000\n",
      "\u001B[1m26/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7626 - loss: 0.6488 \n",
      "Epoch 159: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7630 - loss: 0.6490 - val_accuracy: 0.9584 - val_loss: 0.2914\n",
      "Epoch 160/1000\n",
      "\u001B[1m24/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7758 - loss: 0.6301 \n",
      "Epoch 160: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7750 - loss: 0.6303 - val_accuracy: 0.9559 - val_loss: 0.2965\n",
      "Epoch 161/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7650 - loss: 0.6507 \n",
      "Epoch 161: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7657 - loss: 0.6488 - val_accuracy: 0.9535 - val_loss: 0.2950\n",
      "Epoch 162/1000\n",
      "\u001B[1m26/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7639 - loss: 0.6594 \n",
      "Epoch 162: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7637 - loss: 0.6600 - val_accuracy: 0.9494 - val_loss: 0.2951\n",
      "Epoch 163/1000\n",
      "\u001B[1m26/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7729 - loss: 0.6264 \n",
      "Epoch 163: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - accuracy: 0.7723 - loss: 0.6281 - val_accuracy: 0.9567 - val_loss: 0.2914\n",
      "Epoch 164/1000\n",
      "\u001B[1m26/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7791 - loss: 0.6276 \n",
      "Epoch 164: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7782 - loss: 0.6294 - val_accuracy: 0.9641 - val_loss: 0.2925\n",
      "Epoch 165/1000\n",
      "\u001B[1m26/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7826 - loss: 0.6111 \n",
      "Epoch 165: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7804 - loss: 0.6156 - val_accuracy: 0.9535 - val_loss: 0.3022\n",
      "Epoch 166/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7740 - loss: 0.6069 \n",
      "Epoch 166: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7734 - loss: 0.6090 - val_accuracy: 0.9600 - val_loss: 0.2993\n",
      "Epoch 167/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7643 - loss: 0.6479 \n",
      "Epoch 167: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7651 - loss: 0.6467 - val_accuracy: 0.9518 - val_loss: 0.2955\n",
      "Epoch 168/1000\n",
      "\u001B[1m28/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7816 - loss: 0.6183 \n",
      "Epoch 168: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7815 - loss: 0.6183 - val_accuracy: 0.9510 - val_loss: 0.2935\n",
      "Epoch 169/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7617 - loss: 0.6425 \n",
      "Epoch 169: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7631 - loss: 0.6400 - val_accuracy: 0.9543 - val_loss: 0.2849\n",
      "Epoch 170/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7642 - loss: 0.6527 \n",
      "Epoch 170: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - accuracy: 0.7646 - loss: 0.6506 - val_accuracy: 0.9608 - val_loss: 0.2855\n",
      "Epoch 171/1000\n",
      "\u001B[1m28/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7730 - loss: 0.6305 \n",
      "Epoch 171: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7735 - loss: 0.6291 - val_accuracy: 0.9559 - val_loss: 0.2902\n",
      "Epoch 172/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7487 - loss: 0.6720 \n",
      "Epoch 172: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7505 - loss: 0.6689 - val_accuracy: 0.9592 - val_loss: 0.2842\n",
      "Epoch 173/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7778 - loss: 0.6334 \n",
      "Epoch 173: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7773 - loss: 0.6341 - val_accuracy: 0.9535 - val_loss: 0.2924\n",
      "Epoch 174/1000\n",
      "\u001B[1m28/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7715 - loss: 0.6230 \n",
      "Epoch 174: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - accuracy: 0.7715 - loss: 0.6229 - val_accuracy: 0.9567 - val_loss: 0.2845\n",
      "Epoch 175/1000\n",
      "\u001B[1m28/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7805 - loss: 0.6181 \n",
      "Epoch 175: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7802 - loss: 0.6192 - val_accuracy: 0.9576 - val_loss: 0.2895\n",
      "Epoch 176/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7865 - loss: 0.6123 \n",
      "Epoch 176: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7862 - loss: 0.6126 - val_accuracy: 0.9567 - val_loss: 0.2897\n",
      "Epoch 177/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7800 - loss: 0.6146 \n",
      "Epoch 177: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7790 - loss: 0.6157 - val_accuracy: 0.9527 - val_loss: 0.2906\n",
      "Epoch 178/1000\n",
      "\u001B[1m26/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7787 - loss: 0.6463 \n",
      "Epoch 178: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7781 - loss: 0.6450 - val_accuracy: 0.9535 - val_loss: 0.2996\n",
      "Epoch 179/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7777 - loss: 0.6153 \n",
      "Epoch 179: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7775 - loss: 0.6168 - val_accuracy: 0.9567 - val_loss: 0.2966\n",
      "Epoch 180/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7661 - loss: 0.6511 \n",
      "Epoch 180: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7674 - loss: 0.6478 - val_accuracy: 0.9559 - val_loss: 0.2947\n",
      "Epoch 181/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7636 - loss: 0.6915 \n",
      "Epoch 181: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - accuracy: 0.7645 - loss: 0.6871 - val_accuracy: 0.9616 - val_loss: 0.2872\n",
      "Epoch 182/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7801 - loss: 0.6161 \n",
      "Epoch 182: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - accuracy: 0.7804 - loss: 0.6157 - val_accuracy: 0.9543 - val_loss: 0.2892\n",
      "Epoch 183/1000\n",
      "\u001B[1m24/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7832 - loss: 0.6078 \n",
      "Epoch 183: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7835 - loss: 0.6089 - val_accuracy: 0.9494 - val_loss: 0.2914\n",
      "Epoch 184/1000\n",
      "\u001B[1m28/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7681 - loss: 0.6295 \n",
      "Epoch 184: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7685 - loss: 0.6301 - val_accuracy: 0.9469 - val_loss: 0.2943\n",
      "Epoch 185/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7888 - loss: 0.5987 \n",
      "Epoch 185: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7889 - loss: 0.5990 - val_accuracy: 0.9584 - val_loss: 0.2866\n",
      "Epoch 186/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7695 - loss: 0.6439 \n",
      "Epoch 186: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7691 - loss: 0.6440 - val_accuracy: 0.9633 - val_loss: 0.2904\n",
      "Epoch 187/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7903 - loss: 0.6166 \n",
      "Epoch 187: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7895 - loss: 0.6177 - val_accuracy: 0.9543 - val_loss: 0.2928\n",
      "Epoch 188/1000\n",
      "\u001B[1m24/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7774 - loss: 0.6325 \n",
      "Epoch 188: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.7769 - loss: 0.6328 - val_accuracy: 0.9559 - val_loss: 0.2969\n",
      "Epoch 189/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7811 - loss: 0.6055 \n",
      "Epoch 189: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7805 - loss: 0.6063 - val_accuracy: 0.9518 - val_loss: 0.2937\n",
      "Epoch 190/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7726 - loss: 0.6287 \n",
      "Epoch 190: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7726 - loss: 0.6290 - val_accuracy: 0.9494 - val_loss: 0.2964\n",
      "Epoch 191/1000\n",
      "\u001B[1m28/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7712 - loss: 0.6362 \n",
      "Epoch 191: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.7711 - loss: 0.6365 - val_accuracy: 0.9429 - val_loss: 0.2951\n",
      "Epoch 192/1000\n",
      "\u001B[1m27/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7646 - loss: 0.6383 \n",
      "Epoch 192: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - accuracy: 0.7652 - loss: 0.6378 - val_accuracy: 0.9510 - val_loss: 0.2947\n",
      "Epoch 192: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2067baafb60>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T17:57:21.393739Z",
     "start_time": "2025-04-07T17:57:21.257208Z"
    }
   },
   "source": [
    "# モデル評価\n",
    "val_loss, val_acc = model.evaluate(X_test, y_test, batch_size=128)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m10/10\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.9552 - loss: 0.2872 \n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T17:57:21.541673Z",
     "start_time": "2025-04-07T17:57:21.409832Z"
    }
   },
   "source": [
    "# 保存したモデルのロード\n",
    "model = tf.keras.models.load_model(model_save_path)"
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T17:57:21.696340Z",
     "start_time": "2025-04-07T17:57:21.558296Z"
    }
   },
   "source": [
    "# 推論テスト\n",
    "predict_result = model.predict(np.array([X_test[0]]))\n",
    "print(np.squeeze(predict_result))\n",
    "print(np.argmax(np.squeeze(predict_result)))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 81ms/step\n",
      "[2.3492694e-01 3.1557087e-02 1.3785633e-04 7.2212785e-01 6.1031133e-03\n",
      " 5.1471922e-03]\n",
      "3\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 混同行列"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T17:57:22.230768Z",
     "start_time": "2025-04-07T17:57:21.712246Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "def print_confusion_matrix(y_true, y_pred, report=True):\n",
    "    labels = sorted(list(set(y_true)))\n",
    "    cmx_data = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    \n",
    "    df_cmx = pd.DataFrame(cmx_data, index=labels, columns=labels)\n",
    " \n",
    "    fig, ax = plt.subplots(figsize=(7, 6))\n",
    "    sns.heatmap(df_cmx, annot=True, fmt='g' ,square=False)\n",
    "    ax.set_ylim(len(set(y_true)), 0)\n",
    "    plt.show()\n",
    "    \n",
    "    if report:\n",
    "        print('Classification Report')\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "Y_pred = model.predict(X_test)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "print_confusion_matrix(y_test, y_pred)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 700x600 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.94      0.96       398\n",
      "           1       0.98      0.95      0.96       238\n",
      "           2       0.95      0.99      0.97       327\n",
      "           3       0.83      0.94      0.88        78\n",
      "           4       0.82      0.93      0.87        55\n",
      "           5       0.99      0.91      0.95       129\n",
      "\n",
      "    accuracy                           0.95      1225\n",
      "   macro avg       0.92      0.94      0.93      1225\n",
      "weighted avg       0.95      0.95      0.95      1225\n",
      "\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow-Lite用のモデルへ変換"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T17:57:22.938080Z",
     "start_time": "2025-04-07T17:57:22.337255Z"
    }
   },
   "source": [
    "# モデルを変換(量子化)\n",
    "tflite_save_path = 'model/keypoint_classifier/keypoint_classifier.tflite'\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_quantized_model = converter.convert()\n",
    "\n",
    "open(tflite_save_path, 'wb').write(tflite_quantized_model)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\grimm\\AppData\\Local\\Temp\\tmp0nul8ts8\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\grimm\\AppData\\Local\\Temp\\tmp0nul8ts8\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\grimm\\AppData\\Local\\Temp\\tmp0nul8ts8'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 42), dtype=tf.float32, name='input_layer_1')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 6), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2226867367056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2226867363984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2226867367248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2226867363792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2226867365136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2226895589584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6644"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 推論テスト"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T17:57:22.969612Z",
     "start_time": "2025-04-07T17:57:22.953906Z"
    }
   },
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=tflite_save_path)\n",
    "interpreter.allocate_tensors()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grimm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
      "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T17:59:59.765993Z",
     "start_time": "2025-04-07T17:59:59.761607Z"
    }
   },
   "source": [
    "# 入出力テンソルを取得\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()"
   ],
   "outputs": [],
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T18:00:01.983092Z",
     "start_time": "2025-04-07T18:00:01.977762Z"
    }
   },
   "source": [
    "interpreter.set_tensor(input_details[0]['index'], np.array([X_test[0]]))"
   ],
   "outputs": [],
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-04-07T17:57:23.071900Z",
     "start_time": "2025-04-07T17:57:23.065522Z"
    }
   },
   "source": [
    "%%time\n",
    "# 推論実施\n",
    "interpreter.invoke()\n",
    "tflite_results = interpreter.get_tensor(output_details[0]['index'])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T17:57:23.142339Z",
     "start_time": "2025-04-07T17:57:23.104963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 推論専用のモデルとして保存\n",
    "model.save(model_save_path, include_optimizer=False)"
   ],
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T17:57:23.164939Z",
     "start_time": "2025-04-07T17:57:23.157968Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(np.squeeze(tflite_results))\n",
    "print(np.argmax(np.squeeze(tflite_results)))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.3492685e-01 3.1557113e-02 1.3785659e-04 7.2212791e-01 6.1031166e-03\n",
      " 5.1471950e-03]\n",
      "3\n"
     ]
    }
   ],
   "execution_count": 44
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
